import sys
import os
import numpy as np
import librosa
import torch
import glob
import pickle

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä¸­
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.lsm import AION_LSM_Network
from src.lsm import AION_LSM_Network
from src.config import LSM_N_NEURONS, OBS_SHAPE, AUDIO_SR, HOP_LENGTH, SEED, WEIGHTS_PATH, TRAIN_CHECKPOINT_PATH


def preprocess_audio(audio_path):
    """åŠ è½½éŸ³é¢‘å¹¶è½¬æ¢ä¸º Mel é¢‘è°±åºåˆ—"""
    y, sr = librosa.load(audio_path, sr=AUDIO_SR)
    # æå– Mel é¢‘è°± (n_mels = OBS_SHAPE)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=OBS_SHAPE, hop_length=HOP_LENGTH, n_fft=HOP_LENGTH*2)
    # è½¬ä¸ºå¯¹æ•°åˆ†è´ï¼Œä½¿ç”¨å›ºå®šå‚è€ƒå€¼ 1.0 ä»¥ä¿ç•™ç»å¯¹éŸ³é‡ä¿¡æ¯
    mel_db = librosa.power_to_db(mel, ref=1.0)
    # ä½¿ç”¨å…¨å±€å›ºå®šèŒƒå›´å½’ä¸€åŒ– [-80, 0] dB -> [0, 1]
    mel_norm = (mel_db + 80.0) / 80.0
    mel_norm = np.clip(mel_norm, 0, 1)
    return mel_norm.T # (n_frames, n_mels)

def train(dataset_path, checkpoint_path=TRAIN_CHECKPOINT_PATH, limit=None):

    print(f"æ­£åœ¨å‡†å¤‡æ•°æ®é›†: {dataset_path}")
    wav_files = glob.glob(os.path.join(dataset_path, "**/*.wav"), recursive=True)
    if not wav_files:
        print("âŒ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ï¼è¯·æä¾›æœ‰æ•ˆçš„æ•°æ®é›†è·¯å¾„ã€‚")
        return

    # åˆå§‹åŒ– LSM å’Œ ç´¯ç§¯çŸ©é˜µ
    lsm = AION_LSM_Network()
    n_neurons = LSM_N_NEURONS
    n_out = OBS_SHAPE
    
    sts_total = np.zeros((n_neurons, n_neurons))  # S^T * S
    sty_total = np.zeros((n_neurons, n_out))      # S^T * Y
    processed_files = set()
    total_frames = 0

    # å°è¯•åŠ è½½æ–­ç‚¹ (Checkpoint)
    if os.path.exists(checkpoint_path):
        print(f"ğŸ”„ å‘ç°æ–­ç‚¹æ–‡ä»¶ {checkpoint_path}ï¼Œæ­£åœ¨æ¢å¤è¿›åº¦...")
        checkpoint = torch.load(checkpoint_path)
        sts_total = checkpoint['sts_total']
        sty_total = checkpoint['sty_total']
        processed_files = checkpoint['processed_files']
        total_frames = checkpoint.get('total_frames', 0)
        print(f"å·²æ¢å¤: å·²å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_frames} å¸§ã€‚")

    files_to_process = [f for f in wav_files if f not in processed_files]
    if limit:
        files_to_process = files_to_process[:limit]

    if not files_to_process:
        print("âœ¨ æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæ¯•ï¼Œæˆ–æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")
    else:
        print(f"æ­£åœ¨å¤„ç† {len(files_to_process)} ä¸ªæ–°æ–‡ä»¶...")
        
        try:
            for i, wav_file in enumerate(files_to_process):
                try:
                    mel_seq = preprocess_audio(wav_file)
                    
                    # å›å£°çŠ¶æ€é‡‡é›† (Harvesting States)
                    lsm.reset()
                    
                    # é‡‡é›†è¯¥æ–‡ä»¶çš„ spikes å’Œ targets
                    file_spikes = []
                    file_targets = []
                    for t in range(len(mel_seq) - 1):
                        spikes, _ = lsm.step(mel_seq[t])
                        file_spikes.append(spikes)
                        file_targets.append(mel_seq[t+1])
                    
                    S = np.array(file_spikes)
                    Y = np.array(file_targets)
                    
                    # å¢é‡ç´¯ç§¯çŸ©é˜µ
                    sts_total += S.T @ S
                    sty_total += S.T @ Y
                    total_frames += len(S)
                    processed_files.add(wav_file)
                    
                    # æ¯ 50 ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡ä¸´æ—¶æ–­ç‚¹
                    if (i + 1) % 50 == 0:
                        torch.save({
                            'sts_total': sts_total,
                            'sty_total': sty_total,
                            'processed_files': processed_files,
                            'total_frames': total_frames
                        }, checkpoint_path)
                        print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦: å¤„ç†åˆ°ç¬¬ {len(processed_files)}/{len(wav_files)} ä¸ªæ–‡ä»¶...")
                        
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {wav_file} æ—¶å‡ºé”™: {e}")
                    
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
            torch.save({
                'sts_total': sts_total,
                'sty_total': sty_total,
                'processed_files': processed_files,
                'total_frames': total_frames
            }, checkpoint_path)
            print("è¿›åº¦å·²ä¿å­˜ã€‚æ‚¨å¯ä»¥éšæ—¶é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ã€‚")
            return

    # æœ€ç»ˆè®¡ç®—æƒé‡
    if total_frames > 0:
        print(f"ğŸ—ï¸ æ­£åœ¨æ±‚è§£æœ€ç»ˆæƒé‡ (æ€»å¸§æ•°: {total_frames})...")
        lambda_reg = 1.0
        I = np.eye(n_neurons)
        A = sts_total + lambda_reg * I
        B = sty_total
        
        # æ±‚è§£ A * W_out = B
        W_out = np.linalg.solve(A, B)
        
        # ä¿å­˜æƒé‡
        weights_path = WEIGHTS_PATH
        torch.save(W_out, weights_path)

        
        # åŒæ—¶ä¿å­˜æœ€ç»ˆæ–­ç‚¹ä»¥ä¾¿æœªæ¥ç»§ç»­æ‰©å……æ•°æ®é›†
        torch.save({
            'sts_total': sts_total,
            'sty_total': sty_total,
            'processed_files': processed_files,
            'total_frames': total_frames
        }, checkpoint_path)
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   - æœ€ç»ˆæƒé‡: {weights_path}")
        print(f"   - æ–­ç‚¹çŠ¶æ€: {checkpoint_path}")
    else:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®ã€‚")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="AION ç”Ÿæˆå¼ LSM å¢é‡è®­ç»ƒè„šæœ¬ (æ”¯æŒæ–­ç‚¹ç»­ç»ƒ)")
    parser.add_argument("--data", type=str, help="WAV æ•°æ®é›†ç›®å½•è·¯å¾„")
    parser.add_argument("--limit", type=int, default=None, help="æœ¬æ¬¡è¿è¡Œå¤„ç†çš„æœ€å¤§æ–‡ä»¶æ•°")
    parser.add_argument("--limit", type=int, default=None, help="æœ¬æ¬¡è¿è¡Œå¤„ç†çš„æœ€å¤§æ–‡ä»¶æ•°")
    parser.add_argument("--checkpoint", type=str, default=TRAIN_CHECKPOINT_PATH, help="æ–­ç‚¹æ–‡ä»¶ä¿å­˜è·¯å¾„")
    args = parser.parse_args()

    
    if args.data:
        train(args.data, checkpoint_path=args.checkpoint, limit=args.limit)
    else:
        print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("  python scripts/train_generative.py --data LJSpeech-1.1/wavs")
        print("  æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢å¹¶ä¿å­˜è¿›åº¦ã€‚")
