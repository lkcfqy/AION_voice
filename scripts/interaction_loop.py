import sys
import os
import time
import torch
import numpy as np
import librosa
import sounddevice as sd
import queue
import threading
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.lsm import AION_LSM_Network
from src.adapter import RandomProjectionAdapter
from src.gwt import AttentionGWT
from src.hrr import ResonatorNetwork
from src.mhn import ModernHopfieldNetwork
from src.drive import SocialDrive
from src.dashboard import AIONDashboard
from src.config import LSM_N_NEURONS, OBS_SHAPE, AUDIO_SR, HOP_LENGTH, DEVICE, WEIGHTS_PATH, ADAPTER_SCALING

class AION_Agent_GPU:
    def __init__(self):
        print(f"[!] åˆå§‹åŒ– AION GPU è®¤çŸ¥æ¶æ„ (Device: {DEVICE})...")
        
        # 1. æ ¸å¿ƒæ¨¡å— (Core Modules)
        self.device = DEVICE
        self.lsm = AION_LSM_Network(device=DEVICE)
        self.adapter = RandomProjectionAdapter(device=DEVICE)
        self.gwt = AttentionGWT(device=DEVICE)
        self.mhn = ModernHopfieldNetwork(device=DEVICE)
        self.resonator = ResonatorNetwork(device=DEVICE)
        self.drive = SocialDrive()
        
        # åŠ è½½æƒé‡
        if os.path.exists(WEIGHTS_PATH):
            print(f"[INFO] åŠ è½½æƒé‡: {WEIGHTS_PATH}")
            loaded_tensor = torch.load(WEIGHTS_PATH, map_location=DEVICE)
            self.lsm.W_out.data = loaded_tensor.to(DEVICE)
        else:
            print("[WARNING] æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ– (å°†æ— æ³•æ­£å¸¸è¯´è¯)")

        # 2. çŠ¶æ€ç®¡ç†
        self.running = True
        self.is_sleeping = False
        self.silence_timer = 0
        self.last_activity_time = time.time()
        
        # éŸ³é¢‘ç¼“å†²
        self.chunk_size = HOP_LENGTH
        self.n_fft = self.chunk_size * 2
        self.in_buffer = np.zeros(self.n_fft)
        
        # éŸ³é¢‘å¤„ç†çŸ©é˜µ (CPU -> GPU åœ¨å¾ªç¯ä¸­å¤„ç†)
        self.mel_basis = librosa.filters.mel(sr=AUDIO_SR, n_fft=self.n_fft, n_mels=OBS_SHAPE)
        self.mel_basis_inv = np.linalg.pinv(self.mel_basis)
        
        # ç¡çœ è®¾ç½®
        self.SLEEP_THRESHOLD = 5.0 # ç§’ (æ— å£°å¤šé•¿æ—¶é—´åå…¥ç¡) - å·²ä¿®æ”¹ä¸º5ç§’ä»¥ä¾¿æµ‹è¯•
        
        # ä»ªè¡¨ç›˜
        try:
            self.dashboard = AIONDashboard()
            self.use_dashboard = True
        except:
            print("[WARNING] Dashboard æœªè¿æ¥")
            self.use_dashboard = False

        # ä»æ•°æ®é›†é¢„åŠ è½½è®°å¿†
        self.preload_memories()
            
    def preload_memories(self):
        """ä»æ•°æ®é›†ä¸­åŠ è½½å…ˆå¤©è®°å¿†"""
        import glob
        import random
        
        print("ğŸ“¥ æ­£åœ¨æ¤å…¥å…ˆå¤©è®°å¿† (ä»è®­ç»ƒé›†)...")
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        dataset_pattern = os.path.join(project_root, "datasets", "**", "*.wav")
        print(f"   Searching in: {dataset_pattern}")
        
        wav_files = glob.glob(dataset_pattern, recursive=True)
        print(f"   Found {len(wav_files)} files.")
        
        if not wav_files:
            print("[WARNING] æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ï¼Œå¤§è„‘å°†ä»¥ç©ºç™½çŠ¶æ€å¯åŠ¨ã€‚")
            return
            
        # éšæœºé€‰æ‹© 5 ä¸ªæ–‡ä»¶
        count = 5
        selected_files = random.sample(wav_files, min(len(wav_files), count))
        
        for wav_path in selected_files:
            try:
                # å¿«é€Ÿå¤„ç†æµç¨‹ (ä¸æ’­æ”¾ï¼Œåªè®°å¿†)
                y, sr = librosa.load(wav_path, sr=AUDIO_SR)
                # æˆªå–ä¸€å°æ®µ (1ç§’)
                if len(y) > AUDIO_SR:
                    y = y[:AUDIO_SR]
                
                # å¡«å……ä»¥é˜²è¿‡çŸ­
                if len(y) < self.n_fft:
                    y = np.pad(y, (0, self.n_fft - len(y)))
                    
                mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=OBS_SHAPE, hop_length=self.chunk_size, n_fft=self.n_fft)
                mel_db = librosa.power_to_db(mel, ref=1.0)
                if mel_db.shape[1] > 0:
                    mel_vec = (mel_db[:, -1] + 80.0) / 80.0 # å–æœ€åä¸€å¸§ä½œä¸ºç‰¹å¾
                    mel_vec = np.clip(mel_vec, 0, 1)
                    
                    # è½¬ä¸º Tensor
                    mel_tensor = torch.tensor(mel_vec, dtype=torch.float32, device=self.device)
                    
                    # æ¿€æ´» LSM è·å–è„‰å†²æ¨¡å¼
                    self.lsm.reset()
                    # é¢„çƒ­å‡ æ­¥
                    for _ in range(5):
                        spikes = self.lsm.forward(mel_tensor)
                        
                    # å½¢æˆæ¦‚å¿µå¹¶å­˜å‚¨
                    concept = self.adapter.forward(spikes.flatten())
                    added = self.mhn.add_memory(concept)
                    if added:
                        print(f"   Mapped: {os.path.basename(wav_path)}")
                
            except Exception as e:
                print(f"   Skipped {wav_path}: {e}")
                
        print(f"[BRAIN] æˆåŠŸæ¤å…¥ {self.mhn.memory_count} æ¡å…ˆå¤©è®°å¿†ï¼")

    def audio_callback(self, indata, outdata, frames, time_info, status):
        """å®æ—¶éŸ³é¢‘å›è°ƒ (è¿è¡Œåœ¨ç‹¬ç«‹çº¿ç¨‹)"""
        if self.is_sleeping:
            # ç¡çœ æ¨¡å¼ï¼šä¸å¤„ç†å¤–ç•Œè¾“å…¥ï¼Œåªæ’­æ”¾å†…éƒ¨ç”Ÿæˆçš„â€œæ¢¦è¯â€
            # è¿™é‡Œæˆ‘ä»¬é€šè¿‡ check_dream_queue æˆ–ç±»ä¼¼æœºåˆ¶è·å–è¾“å‡º
            # ç®€å•èµ·è§ï¼Œç¡çœ æ—¶çš„è¾“å‡ºç”± cognitive_loop ç›´æ¥å†™å…¥ sounddevice çš„ OutputStream?
            # æˆ–è€…åœ¨è¿™é‡Œå¡«é›¶ï¼Œç”±ä¸»çº¿ç¨‹æ§åˆ¶æ’­æ”¾ã€‚
            outdata.fill(0)
            return

        # 1. è¾“å…¥å¤„ç† (Input Processing)
        new_data = indata.flatten()
        self.in_buffer = np.roll(self.in_buffer, -self.chunk_size)
        self.in_buffer[-self.chunk_size:] = new_data
        
        # éº¦å…‹é£å¢ç›Š (Pre-amp Gain)
        gain = 100.0
        buffer_boosted = self.in_buffer * gain
        input_rms = np.sqrt(np.mean(buffer_boosted**2))
        
        # æ´»åŠ¨æ£€æµ‹ (é™ä½é˜ˆå€¼)
        if input_rms > 0.02:
            self.last_activity_time = time.time()
        
        # è®¡ç®— Mel é¢‘è°± (CPU)
        mel = librosa.feature.melspectrogram(y=buffer_boosted, sr=AUDIO_SR, n_mels=OBS_SHAPE, hop_length=self.chunk_size, n_fft=self.n_fft)
        mel_db = librosa.power_to_db(mel, ref=1.0)
        mel_vec = (mel_db[:, -1] + 80.0) / 80.0
        mel_vec = np.clip(mel_vec, 0, 1)
        
        # å™ªå£°é—¨ (Noise Gate) (é™ä½é˜ˆå€¼)
        if np.max(mel_vec) < 0.05:
            mel_vec.fill(0)
            
        # 2. ä¼ è¾“åˆ° GPU
        mel_tensor = torch.tensor(mel_vec, dtype=torch.float32, device=self.device)
        
        # 3. LSM æ¨¡æ‹Ÿæ­¥ (GPU)
        # æ³¨å…¥ è‡ªä¸Šè€Œä¸‹ (Top-down) åç½® (æ¥è‡ª GWT å¹¿æ’­)
        bias = self.gwt.workspace_content # (1, D)
        # Adapter åå‘ä¼ æ’­: HDC -> Neurons
        bias_current = None
        if bias is not None:
             bias_current = self.adapter.backward(bias).flatten() # numpy
             bias_current = torch.tensor(bias_current, device=self.device)
             
        # ä¿®å¤é€»è¾‘é”™è¯¯: è¾“å…¥ä¿¡å· (Mel) å’Œ åç½®ç”µæµ (Neuron Space) ç»´åº¦ä¸åŒï¼Œä¸èƒ½ç›´æ¥ç›¸åŠ ã€‚
        # æˆ‘ä»¬ä½¿ç”¨ lsm.forward çš„ external_current å‚æ•°æ³¨å…¥åç½®ã€‚
        scaled_bias = 0.01 * bias_current if bias_current is not None else None
        spikes = self.lsm.forward(mel_tensor, external_current=scaled_bias)
        spikes = spikes.flatten() # (1, N) -> (N,)
        
        # è¯»å‡ºé¢„æµ‹ (Readout)
        prediction = spikes @ self.lsm.W_out # (N,) @ (N, Out) -> (Out,)
        
        # 4. ç¥ç»éŸ³é¢‘åˆæˆ (GPU -> CPU)
        pred_np = prediction.detach().cpu().numpy().flatten()
        pred_np = np.clip(pred_np, 0, 1)
        
        # ä¿¡å·é‡å»º (Mel -> Linear -> Waveform)
        # Mel -> Linear
        mel_db_out = pred_np * 80.0 - 80.0
        mel_p = librosa.db_to_power(mel_db_out)
        stft_p = self.mel_basis_inv @ mel_p
        stft_mag = np.sqrt(np.maximum(stft_p, 0))
        
        # é€†å‚…é‡Œå¶å˜æ¢ (IFFT)
        wav_chunk = np.fft.irfft(stft_mag, n=self.n_fft)
        windowed = wav_chunk * np.hanning(self.n_fft)
        # ç®€åŒ–ç‰ˆ OLA: ç›´æ¥è¾“å‡ºåˆ‡ç‰‡ä¸­å¿ƒéƒ¨åˆ†ä»¥é™ä½å»¶è¿Ÿ
        out_chunk = windowed[:self.chunk_size] 
        
        outdata[:] = np.tanh(out_chunk).reshape(-1, 1) * 1.0

        # æ›´æ–°å…¨å±€çŠ¶æ€ä¾›è®¤çŸ¥å¾ªç¯ä½¿ç”¨
        self.current_spikes = spikes

    def cognitive_cycle(self):
        """æ…¢é€Ÿè®¤çŸ¥å¾ªç¯ (10Hz)"""
        while self.running:
            # ç¡çœ æ£€æŸ¥
            if time.time() - self.last_activity_time > self.SLEEP_THRESHOLD:
                if not self.is_sleeping:
                    print("\n[SLEEP] ç¯å¢ƒå®‰é™ï¼Œè¿›å…¥åœ¨çº¿ç¡çœ å·©å›ºæ¨¡å¼ (åšæ¢¦)...")
                    self.is_sleeping = True
                    self.enter_dream_state()
            else:
                 if self.is_sleeping:
                     print("\n[WAKE] æ£€æµ‹åˆ°æ´»åŠ¨ï¼Œå”¤é†’ä¸­...")
                     self.is_sleeping = False
            
            if not self.is_sleeping:
                # æ­£å¸¸è®¤çŸ¥å¤„ç†
                if hasattr(self, 'current_spikes'):
                    spikes = self.current_spikes # (N,) Tensor
                    
                    # 1. æ„ŸçŸ¥: LSM -> HDC
                    concept = self.adapter.forward(spikes) # (D,)
                    
                    # 2. æ³¨æ„åŠ›å¹¿æ’­
                    # æŸ¥è¯¢ = é©±åŠ¨ (å­¤ç‹¬/éœ€æ±‚) - æš‚æœªå®ç° Drive å‘é‡åŒ–ï¼Œå…ˆç”¨ Concept
                    # å¹¿æ’­: è¿™é‡Œçš„è¾“å…¥æºå¯ä»¥æ˜¯ è§†è§‰, éŸ³é¢‘, è®°å¿†
                    # ç›®å‰åªæœ‰ éŸ³é¢‘ (Concept)
                    broadcast = self.gwt.broadcast(query=concept, input_modules={'audio': concept})
                    
                    # 3. è®°å¿†
                    self.mhn.add_memory(broadcast)
                    
                    # 4. ä»ªè¡¨ç›˜
                    if self.use_dashboard:
                        self.dashboard.update_lsm_raster(torch.where(spikes > 0)[0].cpu().numpy())
                        
            time.sleep(0.1)

    def enter_dream_state(self):
        """åšæ¢¦æ¨¡å¼ï¼šéšæœºå›æ”¾è®°å¿†å¹¶ç”Ÿæˆå£°éŸ³"""
        while self.is_sleeping and self.running:
            # æ£€æµ‹æ˜¯å¦è¢«å”¤é†’
            if time.time() - self.last_activity_time < 0.5:
                break
                
            if self.mhn.memory_count > 0:
                # 1. å›å¿† (éšæœºé‡‡æ ·)
                idx = np.random.randint(0, self.mhn.memory_count)
                memory = self.mhn.memory_matrix[idx] # (D,)
                
                # 2. æƒ³è±¡ (Top-down)
                # HDC -> LSM Neurons
                bias = self.adapter.backward(memory) # (N,) numpy
                bias_tensor = torch.tensor(bias, device=self.device)
                
                # 3. æ¿€æ´» LSM (æ— è¾“å…¥ï¼Œåªæœ‰ bias)
                # æ¨¡æ‹Ÿä¸€æ®µ "æ¢¦å¢ƒ" (ä¾‹å¦‚ 100ms)
                print(f"\r[DREAM] æ­£åœ¨å›æ”¾è®°å¿†ç‰‡æ®µ #{idx}...", end="")
                
                generated_audio = []
                # é‡ç½® LSM å†…éƒ¨çŠ¶æ€ä»¥è·å¾—æ¸…æ™°çš„æ¢¦å¢ƒ
                self.lsm.reset()
                
                for _ in range(10): # 10 å¸§
                    spikes = self.lsm.forward(bias_tensor * 2.0) # å¼ºåˆºæ¿€
                    pred = spikes @ self.lsm.W_out
                    
                    # åˆæˆéŸ³é¢‘
                    p = pred.detach().cpu().numpy()
                    p = np.clip(p, 0, 1)
                    # ... (ç®€å•çš„åˆæˆï¼Œç±»ä¼¼äºå›è°ƒå‡½æ•°)
                    mel_p = librosa.db_to_power(p * 80 - 80)
                    wav = np.fft.irfft(np.sqrt(np.maximum(self.mel_basis_inv @ mel_p, 0)), n=self.n_fft)
                    generated_audio.append(wav[:self.chunk_size])
                    
                # æ’­æ”¾æ¢¦å¢ƒå£°éŸ³
                full_dream = np.concatenate(generated_audio)
                sd.play(np.tanh(full_dream) * 0.5, AUDIO_SR)
                sd.wait()
                
            else:
                print("\r[BRAIN] è®°å¿†åº“ä¸ºç©ºï¼è¯·å…ˆå¯¹ç€éº¦å…‹é£è¯´è¯ï¼Œè®©æˆ‘ç§¯ç´¯ä¸€äº›ç´ æ...", end="")
                time.sleep(1.0)
                
            time.sleep(1.0) # æ¢¦å¢ƒé—´éš”

    def run(self):
        cog_thread = threading.Thread(target=self.cognitive_cycle)
        cog_thread.daemon = True
        cog_thread.start()
        
        print("\n" + "="*50)
        print("[MIC] AION è¯­éŸ³äº¤äº’ç³»ç»Ÿå·²å¯åŠ¨")
        print("[TIP] ä½¿ç”¨æŒ‡å—:")
        print("1. å¯¹ç€éº¦å…‹é£è¯´è¯ -> å®ƒä¼šå­¦ä¹ å¹¶å°è¯•è·Ÿéšä½ çš„å£°éŸ³ã€‚")
        print("2. ä¿æŒå®‰é™ 5 ç§’ -> å®ƒä¼šè¿›å…¥æ¢¦å¢ƒï¼Œå›æ”¾åˆšæ‰å­¦åˆ°çš„å£°éŸ³ç‰‡æ®µã€‚")
        print("[WARNING] æ³¨æ„ï¼šå¯åŠ¨æ—¶è®°å¿†æ˜¯ç©ºçš„ï¼Œä½ å¿…é¡»å…ˆè¯´è¯ï¼")
        print("="*50 + "\n")
        
        print("[MIC] éº¦å…‹é£ç›‘å¬ä¸­...")
        with sd.Stream(samplerate=AUDIO_SR, blocksize=self.chunk_size, channels=1, callback=self.audio_callback):
            while self.running:
                try:
                    time.sleep(1.0)
                except KeyboardInterrupt:
                    self.running = False
                    print("\n[EXIT] æ­£åœ¨åœæ­¢...")

if __name__ == "__main__":
    agent = AION_Agent_GPU()
    agent.run()
