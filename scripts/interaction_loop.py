import sys
import os
import time
import torch
import numpy as np

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä¸­
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.voice_body.environment import AudioEnvironment
from src.lsm import AION_LSM_Network
from src.adapter import RandomProjectionAdapter
from src.voice_body.motor_cortex import MotorCortex
from src.drive import SocialDrive
from src.gwt import GlobalWorkspace
from src.hrr import HDCWorldModel
from src.dashboard import AIONDashboard
from src.mhn import ModernHopfieldNetwork
from src.config import OBS_SHAPE, LSM_STEPS_PER_SAMPLE

class InteractionAgent:
    def __init__(self, device='cpu'):
        self.device = device
        print("æ­£åœ¨åˆå§‹åŒ– AION è¯­éŸ³æ™ºèƒ½ä½“ï¼ˆè¿åŠ¨åŠ¨åŠ›å­¦ç‰ˆï¼‰...")
        
        # 1. èº«ä½“ä¸ç¯å¢ƒ (Body & Environment)
        try:
            import sounddevice as sd
            sd.query_devices(kind='input')
            self.env = AudioEnvironment(use_microphone=True)
            print("âœ… éº¦å…‹é£å’ŒéŸ³ç®±åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception:
            print("âš ï¸ ç¡¬ä»¶åˆå§‹åŒ–å¤±è´¥ã€‚æ­£åœ¨å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼ (SIMULATION)ã€‚")
            self.env = AudioEnvironment(use_microphone=False)
            
        # 2. æ„ŸçŸ¥ (è€³èœ— - Ear)
        self.lsm = AION_LSM_Network()
        self.adapter = RandomProjectionAdapter()
        
        # 3. å¤§è„‘ (è®°å¿†ä¸å…¨å±€å·¥ä½œç©ºé—´ - Brain, Memory & GWT)
        self.gwt = GlobalWorkspace(device=device)
        self.drive = SocialDrive()
        self.wm = HDCWorldModel(n_actions=1, device=device)
        
        if os.path.exists("association_memory.pt"):
            print("æ­£åœ¨åŠ è½½å…³è”è®°å¿†...")
            self.wm.M_per_action = torch.load("association_memory.pt")
            
        # 4. åŠ¨ä½œ (è¿åŠ¨çš®å±‚ - Motor Cortex)
        self.motor = MotorCortex()
        if os.path.exists("motor_cortex_weights.pt"):
            self.motor.load_weights("motor_cortex_weights.pt")
        
        # 5. æƒ…èŠ‚è®°å¿† (MHN)
        print("æ­£åœ¨åˆå§‹åŒ–æƒ…èŠ‚è®°å¿† (MHN)...")
        self.memory = ModernHopfieldNetwork(device=device)

        # 6. æ§åˆ¶é¢æ¿ (Dashboard)
        try:
            self.dashboard = AIONDashboard()
            print("âœ… æ§åˆ¶é¢æ¿å·²è¿æ¥ã€‚")
        except Exception:
            self.dashboard = None

        self.state = "LISTEN" 
        self.silence_counter = 0

    def run(self):
        print("\n=== AION è¿åŠ¨äº¤äº’å¾ªç¯å·²å¯åŠ¨ ===")
        print("æŒ‰ Ctrl+C åœæ­¢ã€‚")
        try:
            while True:
                if self.state == "LISTEN":
                    # å€¾å¬æ—¶æ„å›¾ä¸º None
                    obs, _, _, _, _ = self.env.step(intent_vector=None)
                    activity = np.mean(obs)
                    
                    if activity > 0.05: 
                        print(f"ğŸ‘‚ å¬åˆ°å£°éŸ³ (å¼ºåº¦: {activity:.2f})")
                        spikes = self.process_hearing(obs)
                        self.state = "PONDER"
                        self.silence_counter = 0
                        self.drive.step(heard_voice=True)

                        if self.dashboard:
                            self.dashboard.update_env_view(obs)
                            self.dashboard.update_lsm_raster(spikes)

                        current_concept = self.gwt.current_sense
                        self.memory.add_memory(current_concept)
                    else:
                        self.silence_counter += 1
                        time.sleep(0.1)
                        self.drive.step(heard_voice=False)
                        
                        if self.drive.loneliness > 0.5 and self.silence_counter > 50:
                            print("ğŸ˜ æ„Ÿåˆ°å¯‚å¯... æ­£åœ¨ä¸»åŠ¨å¼€å¯å¯¹è¯ã€‚")
                            self.state = "SPEAK_INITIATIVE"
                            
                elif self.state == "PONDER":
                    current_concept = self.gwt.current_sense
                    print("ğŸ¤” æ€è€ƒä¸­...")
                    
                    energy = self.memory.compute_energy(current_concept)
                    recalled = self.memory.retrieve(current_concept)
                    
                    if self.dashboard:
                        self.dashboard.update_energy(energy)
                        sim = torch.nn.functional.cosine_similarity(current_concept, recalled, dim=-1).item()
                        self.dashboard.update_hdc_similarity(sim)

                    reply_concept = self.wm.predict(current_concept, 0)
                    self.gwt.update_pred(reply_concept)
                    self.state = "SPEAK"
                    
                elif self.state == "SPEAK":
                    print("ğŸ—£ï¸ æ­£åœ¨å›å¤...")
                    concept = self.gwt.current_pred
                    self.speak(concept)
                    self.state = "LISTEN"
                    
                elif self.state == "SPEAK_INITIATIVE":
                    print("ğŸ—£ï¸ æ­£åœ¨ä¸»åŠ¨å‘èµ·å¯¹è¯...")
                    fake_concept = torch.randn(10000).to(self.device).sign()
                    fake_concept[fake_concept==0] = 1.0
                    self.speak(fake_concept)
                    self.state = "LISTEN"
                    self.silence_counter = 0

                if self.dashboard:
                    surprise = self.gwt.compute_surprise()
                    free_energy = self.drive.compute_free_energy(surprise)
                    self.dashboard.update_survival(free_energy=free_energy, loneliness=self.drive.loneliness)
                    
        except KeyboardInterrupt:
            print("\nå·²åœæ­¢ã€‚")

    def process_hearing(self, obs):
        self.lsm.reset()
        spikes_accumulated = np.zeros(self.lsm.n_neurons)
        for _ in range(LSM_STEPS_PER_SAMPLE):
            spikes_accumulated += self.lsm.step(obs)
            
        concept = self.adapter.forward(torch.from_numpy(spikes_accumulated.copy()).float())
        concept = concept.to(self.device)
        self.gwt.update_sense(concept)
        return spikes_accumulated
        
    def speak(self, concept):
        """é€šè¿‡è¿åŠ¨çš®å±‚ç”Ÿæˆå¹¶æ’­æ”¾å£°éŸ³ã€‚"""
        # ç¯å¢ƒç±»å¤„ç†éŸ³ç®±é€»è¾‘
        self.env.step(intent_vector=concept)
        self.drive.step(spoke=True)

if __name__ == "__main__":
    agent = InteractionAgent()
    agent.run()
