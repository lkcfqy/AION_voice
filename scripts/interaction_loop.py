import sys
import os
import time
import torch
import numpy as np
import librosa
import sounddevice as sd
import queue
import threading

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.lsm import AION_LSM_Network
from src.adapter import RandomProjectionAdapter
from src.gwt import GlobalWorkspace
from src.drive import SocialDrive
from src.hrr import HDCWorldModel
from src.mhn import ModernHopfieldNetwork
from src.dashboard import AIONDashboard
from src.config import LSM_N_NEURONS, OBS_SHAPE, AUDIO_SR, HOP_LENGTH, SEED, LSM_STEPS_PER_SAMPLE, WEIGHTS_PATH


class IntegratedAIONAgent:
    def __init__(self, device='cpu'):
        self.device = device
        print("æ­£åœ¨åˆå§‹åŒ– AION å®Œæ•´è®¤çŸ¥é›†æˆæ™ºèƒ½ä½“...")
        
        # 1. åŠ¨åŠ›å±‚ (LSM) & æ„ŸçŸ¥é€‚é…å™¨
        # 1. åŠ¨åŠ›å±‚ (LSM) & æ„ŸçŸ¥é€‚é…å™¨
        self.lsm = AION_LSM_Network()
        if os.path.exists(WEIGHTS_PATH):
            print(f"æ­£åœ¨åŠ è½½ LSM è¯»å‡ºå±‚æƒé‡ ({WEIGHTS_PATH})...")
            self.lsm.W_out = torch.load(WEIGHTS_PATH)
        else:
             print(f"âš ï¸ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ {WEIGHTS_PATH}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")

        self.adapter = RandomProjectionAdapter(device=device)

        # 2. æ§åˆ¶å±‚ä¸å­˜å‚¨å±‚ (GWT, HDC, MHN, Drive)
        self.gwt = GlobalWorkspace(device=device)
        self.drive = SocialDrive()
        self.wm = HDCWorldModel(n_actions=1, device=device)
        self.gwt = GlobalWorkspace(device=device)
        self.drive = SocialDrive()
        self.wm = HDCWorldModel(n_actions=1, device=device)
        self.memory = ModernHopfieldNetwork(device=device)
        
        # Dashboard é›†æˆ
        try:
             self.dashboard = AIONDashboard()
             self.use_dashboard = True
             print("âœ… Visdom ä»ªè¡¨ç›˜è¿æ¥æˆåŠŸã€‚")
        except Exception as e:
             print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Visdom æœåŠ¡å™¨ ({e})ã€‚ä»ªè¡¨ç›˜å°†è¢«ç¦ç”¨ã€‚")
             print("   è¯·è¿è¡Œ 'python -m visdom.server' ä»¥å¯ç”¨å¯è§†åŒ–ã€‚")
             self.use_dashboard = False
        
        # 3. çŠ¶æ€åŒæ­¥ä¸å¤šçº¿ç¨‹
        self.input_queue = queue.Queue()
        self.chunk_size = HOP_LENGTH
        self.feedback_factor = 0.05   # å¤§å¹…è°ƒä½åé¦ˆï¼Œç”± 0.3 é™è‡³ 0.05 ä»¥é˜²æ­¢éœ‡è¡
        self.last_prediction = np.zeros(OBS_SHAPE)
        self.volume_factor = 0.5      # å…¨å±€éŸ³é‡ç¼©æ”¾
        self.alpha_smooth = 0.7       # é¢‘è°±å¹³æ»‘ç³»æ•°
        self.prev_audio_tail = np.zeros(self.chunk_size) # ç”¨äºå¹³æ»‘è¡”æ¥
        
        # 4. é¢„è®¡ç®—éŸ³é¢‘åˆæˆæ‰€éœ€çš„ Mel çŸ©é˜µ
        # ç”¨äºå°† Mel è½¬æ¢å› STFT å¹…åº¦
        self.mel_basis = librosa.filters.mel(sr=AUDIO_SR, n_fft=self.chunk_size*2, n_mels=OBS_SHAPE)
        self.mel_basis_inv = np.linalg.pinv(self.mel_basis)
        
        # å…±äº«çŠ¶æ€ï¼ˆç”¨äºè·¨çº¿ç¨‹é€šè®¯ï¼‰
        self.shared_state = {
            'cognitive_bias': np.zeros(LSM_N_NEURONS),
            'last_spikes': np.zeros(LSM_N_NEURONS),
            'dopamine': 0.0,
            'running': True
        }
        
    def audio_callback(self, indata, outdata, frames, time, status):
        """å¿«é€Ÿç‰©ç†ç¯ (32ms å»¶è¿Ÿ)"""
        # A. æ„ŸçŸ¥è¾“å…¥
        y = indata.flatten()
        mel = librosa.feature.melspectrogram(y=y, sr=AUDIO_SR, n_mels=OBS_SHAPE, hop_length=self.chunk_size, n_fft=self.chunk_size*2)
        # è½¬ä¸ºå¯¹æ•°åˆ†è´ï¼Œä½¿ç”¨å›ºå®šå‚è€ƒå€¼ 1.0 (å¿…é¡»ä¸è®­ç»ƒä¸€è‡´)
        mel_db = librosa.power_to_db(mel, ref=1.0)
        # ç¡®ä¿å½¢çŠ¶ä¸º (OBS_SHAPE,)ï¼Œå–æ‰€æœ‰æ—¶é—´å¸§çš„å¹³å‡å€¼
        mel_vec = np.mean(mel_db, axis=1)
        # å½’ä¸€åŒ– (ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„ [-80, 0] æ˜ å°„)
        mel_vec = (mel_vec + 80) / 80.0
        mel_vec = (mel_vec + 80) / 80.0
        mel_vec = np.clip(mel_vec, 0, 1)

        # å®æ—¶æ›´æ–°è€³èœ—è§†å›¾ (å¦‚æœå¯ç”¨ Dashboard)
        if self.use_dashboard and hasattr(self, 'dashboard'):
             # å‘é€è¿™ä¸€å¸§çš„ Mel é¢‘è°±
             # ä¸ºäº†æ˜¾ç¤ºå¥½çœ‹ï¼Œå°†å…¶ä» (OBS_SHAPE,) æ‰©å±•ä¸º (OBS_SHAPE, 1, 1) æˆ–ç±»ä¼¼çš„å›¾åƒæ ¼å¼
             # Dashboard æœŸæœ› (H, W, 3) 
             # ç®€å•çš„å¯è§†åŒ–ï¼šå°†å‘é‡æ‰©å±•ä¸ºæ¡å½¢å›¾
             pass # åœ¨ cognitive loop æ›´æ–°å¯èƒ½æ›´å¥½ï¼Œæˆ–è€…åœ¨è¿™é‡Œæ›´æ–° fast update
             # ç”±äº audio callback é¢‘ç‡å¾ˆé«˜ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦é™é‡‡æ ·
             # æš‚æ—¶åªåœ¨ dashboard ç±»ä¸­åšé¢‘è°±å›¾ç´¯ç§¯ï¼Ÿ
             # Dashboard çš„ update_env_view æœŸæœ›å›¾åƒã€‚
             # æˆ‘ä»¬å¯ä»¥ç®€å•åœ°æŠŠ mel vector æ„é€ æˆä¸€ä¸ªçƒ­åŠ›å›¾æ¡
             
             # æ„é€ ä¸€ä¸ª å›¾åƒ (OBS_SHAPE, 10, 3) ç”¨ä¼ªå½©è‰²
             # ç®€ä¾¿èµ·è§ï¼Œåªåœ¨ cognitive loop æ›´æ–°æ…¢é€Ÿä¿¡æ¯ã€‚
             pass

        # å¢åŠ å™ªå£°é—¨ (Noise Gate): å¦‚æœè¾“å…¥ä¿¡å·å¤ªå¼±ï¼Œç›´æ¥ç½®é›¶
        if np.mean(mel_vec) < 0.1: # ç¨å¾®è°ƒé«˜ä¸€ç‚¹é—¨é™
            mel_vec.fill(0)

        # B. æ³¨å…¥ä¸ç”Ÿæˆ (æ¥å—è®¤çŸ¥åç½®å’Œå¤šå·´èƒºè°ƒèŠ‚)
        bias = self.shared_state['cognitive_bias']
        dopamine = self.shared_state['dopamine']
        
        spikes, next_mel = self.lsm.step(mel_vec + (self.feedback_factor * self.last_prediction), 
                                         dopamine=dopamine, 
                                         cognitive_bias=bias)
                                         
        # å¢åŠ å¹³æ»‘ï¼šé¿å…é¢„æµ‹å€¼è·³å˜å‰§çƒˆå¯¼è‡´æ»‹æ»‹å£°
        self.last_prediction = self.alpha_smooth * self.last_prediction + (1 - self.alpha_smooth) * next_mel
        self.shared_state['last_spikes'] = spikes # æ›´æ–°è„‰å†²çŠ¶æ€ä¾›é€»è¾‘ç¯é‡‡æ ·

        # C. æ’­æ”¾è¾“å‡º
        # å°† Mel å½’ä¸€åŒ– DB è½¬å›è¿‘ä¼¼å¹…åº¦ (é™åˆ¶åœ¨ 0-1 èŒƒå›´å†…é˜²æ­¢çˆ†ç‚¸)
        next_mel_safe = np.clip(next_mel, 0, 1)
        mel_db = next_mel_safe * 80.0 - 80.0
        mel_power = librosa.db_to_power(mel_db)
        
        # æ‰‹åŠ¨è½¬å› STFT å¹…åº¦ (Linear)
        stft_power = self.mel_basis_inv @ mel_power
        stft_mag = np.sqrt(np.maximum(stft_power, 0))
        
        # ä½¿ç”¨ ISTFT è¿›è¡Œå®æ—¶åˆæˆ (é›¶ä½ç”±äºæ— ç›¸ä½ä¿¡æ¯)
        audio_out = librosa.istft(stft_mag.reshape(-1, 1), 
                                 hop_length=self.chunk_size, 
                                 win_length=self.chunk_size*2,
                                 length=self.chunk_size)
        
        # å†™å…¥è¾“å‡ºæµï¼Œä½¿ç”¨ tanh è¿›è¡Œè½¯å‰ªåˆ‡å¹¶åº”ç”¨éŸ³é‡å› å­
        audio_final = np.tanh(audio_out) * self.volume_factor
        outdata[:] = audio_final.reshape(-1, 1)

    def cognitive_loop(self):
        """æ…¢é€Ÿé€»è¾‘ç¯ (çº¦ 100ms å‘¨æœŸ)"""
        print("ğŸ§  è®¤çŸ¥é€»è¾‘ç¯å·²å¯åŠ¨ã€‚")
        while self.shared_state['running']:
            # 1. é‡‡æ · LSM è„‰å†²å¹¶æŠ•å½±åˆ° HDC ç©ºé—´
            spikes = self.shared_state['last_spikes']
            if np.any(spikes):
                # å°†è„‰å†²è½¬æ¢ä¸º HDC æ¦‚å¿µ
                concept = self.adapter.forward(spikes)
                self.gwt.update_sense(concept)
                
                # 2. æƒ…èŠ‚è®°å¿†æ£€ç´¢ä¸èƒ½é‡è®¡ç®— (FEP ç›¸å…³)
                energy = self.memory.compute_energy(concept)
                self.memory.add_memory(concept)
                
                # 3. ç›®æ ‡é©±åŠ¨ä¸æƒŠè®¶åº¦è®¡ç®—
                surprise = self.gwt.compute_surprise()
                self.drive.step(heard_voice=(np.mean(spikes) > 0.1))
                
                # 4. ç”Ÿæˆæ„å›¾ (Top-down Intent)
                # é¢„æµ‹ä¸‹ä¸€åˆ»çš„é«˜ç»´æ¦‚å¿µ
                intent_concept = self.wm.predict(concept, 0)
                self.gwt.update_pred(intent_concept)
                
                # 5. åå‘æŠ•å½±ï¼šå°†â€œæ„å›¾â€è½¬åŒ–ä¸º LSM çš„ç‰©ç†åç½®
                bias = self.adapter.backward(intent_concept)
                self.shared_state['cognitive_bias'] = bias
                
                # 6. å¤šå·´èƒºè°ƒèŠ‚ (åŸºäºæƒŠå–œåº¦å’Œå­¤ç‹¬æ„Ÿ)
                # äº§ç”Ÿçš„å¤šå·´èƒºä¼šå½±å“ LSM çš„ 3-å› å­å­¦ä¹ 
                # 6. å¤šå·´èƒºè°ƒèŠ‚ (åŸºäºæƒŠå–œåº¦å’Œå­¤ç‹¬æ„Ÿ)
                # äº§ç”Ÿçš„å¤šå·´èƒºä¼šå½±å“ LSM çš„ 3-å› å­å­¦ä¹ 
                self.shared_state['dopamine'] = 0.1 if surprise < 0.2 else -0.05
            
            # 7. æ›´æ–° Dashboard
            if self.use_dashboard:
                 # è€³èœ—è§†å›¾ (å½“å‰è¾“å…¥) -> éœ€è¦ä» callback è·å–ä¸€ä»½å‰¯æœ¬
                 # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬æš‚æ—¶åªæ›´æ–°é€»è¾‘çŠ¶æ€
                 
                 # LSM å…‰æ …å›¾
                 active_neurons = np.where(spikes > 0)[0]
                 self.dashboard.update_lsm_raster(active_neurons)
                 
                 # HDC ç›¸ä¼¼åº¦ (Surprise çš„åé¢æˆ– Goal Delta)
                 self.dashboard.update_hdc_similarity(1.0 - surprise) # ç›¸ä¼¼åº¦è¶Šé«˜ï¼ŒæƒŠå–œåº¦è¶Šä½
                 
                 # èƒ½é‡ / é©±åŠ¨
                 free_energy = self.drive.compute_free_energy(surprise)
                 self.dashboard.update_energy(free_energy)
                 self.dashboard.update_survival(free_energy, self.drive.loneliness)
            
            time.sleep(0.1)

    def run(self):
        print("\n=== AION å®Œæ•´è®¤çŸ¥é›†æˆäº¤äº’å·²å¯åŠ¨ ===")
        print("æ¶æ„ï¼šGWT + HDC + MHN + FEP + Generative LSM")
        print("æŒ‰ Ctrl+C åœæ­¢ã€‚")
        
        # å¯åŠ¨è®¤çŸ¥çº¿ç¨‹
        cog_thread = threading.Thread(target=self.cognitive_loop)
        cog_thread.daemon = True
        cog_thread.start()
        
        try:
            with sd.Stream(samplerate=AUDIO_SR,
                           blocksize=self.chunk_size,
                           channels=1,
                           callback=self.audio_callback):
                while True:
                    time.sleep(0.1)
        except KeyboardInterrupt:
            print("\næ­£åœ¨åœæ­¢...")
            self.shared_state['running'] = False
            cog_thread.join(timeout=1.0)
            print("å·²åœæ­¢ã€‚")

if __name__ == "__main__":
    agent = IntegratedAIONAgent()
    agent.run()
